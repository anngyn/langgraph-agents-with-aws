{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5789bc3-b1ae-42c7-94a8-2ef4f89946fc",
   "metadata": {},
   "source": [
    "# Lab 4: Persistence and Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b724f8",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We begin by establishing our agent environment. This process involves loading the necessary environment variables, importing required modules, initializing our Tavily search tool, defining the agent state, and finally, constructing our agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-02 10:59:56,207] p3563 {utils.py:46} INFO - TAVILY_API_KEY variable correctly retrieved from the .env file.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "# import local modules\n",
    "dir_current = os.path.abspath(\"\")\n",
    "dir_parent = os.path.dirname(dir_current)\n",
    "if dir_parent not in sys.path:\n",
    "    sys.path.append(dir_parent)\n",
    "from utils import utils\n",
    "\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Set basic configs\n",
    "logger = utils.set_logger()\n",
    "pp = utils.set_pretty_printer()\n",
    "\n",
    "# Load environment variables from .env file or Secret Manager\n",
    "_ = load_dotenv(\"../.env\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "tavily_ai_api_key = utils.get_tavily_api(\"TAVILY_API_KEY\", aws_region)\n",
    "\n",
    "\n",
    "# Create a bedrock runtime client\n",
    "bedrock_rt = boto3.client(\n",
    "    \"bedrock-runtime\", region_name=aws_region, config=bedrock_config\n",
    ")\n",
    "\n",
    "# Create a bedrock client to check available models\n",
    "bedrock = boto3.client(\"bedrock\", region_name=aws_region, config=bedrock_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da06a64f-a2d5-4a66-8090-9ada0930c684",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989adc7",
   "metadata": {},
   "source": [
    "## Implementing Persistence\n",
    "\n",
    "We now turn our attention to implementing persistence. To achieve this, we introduce the concept of a checkpointer in LangGraph. The checkpointer's function is to create state snapshots after and between each node in our agent's processing graph.\n",
    "\n",
    "#RESOURCE For a more comprehensive understanding of LangGraph's capabilities and usage, refer to the official LangGraph documentation.\n",
    "\n",
    "In this implementation, we utilize a SQLite saver as our checkpointer. This lightweight solution leverages SQLite, a built-in database engine. While we use an in-memory database for this demonstration, it's important to note that this can be easily adapted to connect to an external database for production environments. LangGraph also supports other persistence solutions, including Redis and Postgres, for scenarios requiring more robust database systems.\n",
    "\n",
    "After initializing the checkpointer, we pass it to the `graph.compile` method. We've enhanced our agent to accept a `checkpointer` parameter, which we set to our memory object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01441e5e",
   "metadata": {},
   "source": [
    "## The Agent Class: A Detailed Examination\n",
    "\n",
    "The `Agent` class serves as the cornerstone of our implementation, orchestrating the interactions between the language model (Claude), tools (such as the Tavily search), and the overall conversation flow. Let's examine its key components:\n",
    "\n",
    "1. `__init__` method: This initializer sets up the agent with a model, tools, checkpointer, and an optional system message. It constructs the state graph that defines the agent's behavior.\n",
    "\n",
    "2. `call_bedrock` method: This method is responsible for invoking the Claude model via Amazon Bedrock. It processes the current state (messages) and returns the model's response.\n",
    "\n",
    "3. `exists_action` method: This method evaluates whether the latest message from the model includes any tool calls (actions to be executed).\n",
    "\n",
    "4. `take_action` method: This method executes any tool calls specified by the model and returns the results.\n",
    "\n",
    "The `Agent` class utilizes a `StateGraph` to manage the conversation flow, enabling complex interactions while maintaining a clear and manageable structure. This design choice facilitates the implementation of persistence and streaming capabilities.\n",
    "\n",
    "## Streaming Implementation\n",
    "\n",
    "With our agent now configured, we can implement streaming functionality. There are two primary aspects of streaming to consider:\n",
    "\n",
    "1. Message Streaming: This involves streaming individual messages, including the AI message that determines the next action and the observation message that represents the action's result.\n",
    "\n",
    "2. Token Streaming: This involves streaming each token of the language model's response as it's generated.\n",
    "\n",
    "We'll begin by implementing message streaming. We create a human message (e.g., \"What is the weather in SF?\") and introduce a thread config. This thread config is crucial for managing multiple conversations simultaneously within the persistent checkpointer, a necessity for production applications serving multiple users.\n",
    "\n",
    "We invoke the graph using the `stream` method instead of `invoke`, passing our messages dictionary and thread config. This returns a stream of events representing real-time updates to the state.\n",
    "\n",
    "Upon execution, we observe a stream of results: first, an AI message from Claude determining the action to take, followed by a tool message containing the Tavily search results, and finally, another AI message from Claude answering our initial query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_bedrock)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", self.exists_action, {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_bedrock(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(\n",
    "                ToolMessage(tool_call_id=t[\"id\"], name=t[\"name\"], content=str(result))\n",
    "            )\n",
    "        print(\"Back to the model!\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workshop/langgraph-agents-with-amazon-bedrock/.venv/lib/python3.12/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The class `ChatBedrockConverse` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    client=bedrock_rt,\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=[{'type': 'tool_use', 'name': 'tavily_search_results_json', 'input': {'query': 'weather in san francisco'}, 'id': 'tooluse_7WYCoEagQTCH6RVnxIbxVg'}], response_metadata={'ResponseMetadata': {'RequestId': '80f157db-4874-4008-a2a3-4d4c11668aef', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 02 Oct 2025 07:05:14 GMT', 'content-type': 'application/json', 'content-length': '334', 'connection': 'keep-alive', 'x-amzn-requestid': '80f157db-4874-4008-a2a3-4d4c11668aef'}, 'RetryAttempts': 0}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': 709}}, id='run-f6e0cf5c-0ec5-4244-9bd5-f60bcc69d73a-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'tooluse_7WYCoEagQTCH6RVnxIbxVg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1499, 'output_tokens': 61, 'total_tokens': 1560})]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'tooluse_7WYCoEagQTCH6RVnxIbxVg', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "[ToolMessage(content=\"[{'url': 'https://world-weather.info/forecast/usa/san_francisco/february-2025/', 'content': '# Weather in San Francisco in February 2025 * +59° * +59° 15 mph S 30 inHg94 %07:12 am05:34 pm +61° +57° 21 mph S 29.8 inHg80 %07:11 am05:35 pm +59° +57° +55° +46° +59° +46° 27.7 mph S 29.6 inHg95 %07:08 am05:39 pm +55° +52° +55° +46° +55° +45° +55° +43° +52° +43° +54° +45° +57° +52° +55° +54° +57° +45° 6.5 mph S 30.2 inHg76 %06:58 am05:49 pm +57° +48° +57° +50° +59° +48° +59° +50° +63° +48° +63° +46° +59° +46° +61° +50° +61° +54° +63° +52° +68° +48° +70° +50° +66° +54° Weather in Washington, D.C.**+59°** Sacramento**+72°** Pleasanton**+72°** Redwood City**+72°** San Leandro**+70°** San Mateo**+68°** San Rafael**+63°** San Ramon**+68°** South San Francisco**+68°** Vallejo**+66°** Pacifica**+64°** Berkeley**+66°** Concord**+68°** Daly City**+66°** Temperature units'}, {'url': 'https://weatherspark.com/h/m/557/2025/2/Historical-Weather-in-February-2025-in-San-Francisco-California-United-States', 'content': 'San Francisco Temperature History February 2025 Winter 2025  Link  Download  Compare  Averages  History:JFebMAMJJASOND2024202320222021  The daily range of reported temperatures (gray bars) and 24-hour highs (red ticks) and lows (blue ticks), placed over the daily average high (faint red line) and low (faint blue line) temperature, with 25th to 75th and 10th to 90th percentile bands. Hourly Temperature in February 2025 in San Francisco Winter 2025  Link  Download  Compare  Averages  History:JFebMAMJJASOND2024202320222021  *frigid*   *15°F*   *freezing*   *32°F*   *very cold*   *45°F*   *cold*   *55°F*   *cool*   *65°F*   *comfortable*   *75°F*   *warm*   *85°F*   *hot*   *95°F*   *sweltering*  The hourly reported temperature, color coded into bands. Hourly Wind Direction in 2025 in San Francisco Winter 2025  Link  Download  Compare  Averages  History:JFebMAMJJASOND2024202320222021  *calm**north**east**south**west*  The hourly reported wind direction, color coded by compass point.'}]\", name='tavily_search_results_json', tool_call_id='tooluse_7WYCoEagQTCH6RVnxIbxVg')]\n",
      "[AIMessage(content='Based on the search results, the weather in San Francisco in February 2025 is expected to be:\\n\\n- Temperatures ranging from around 46°F to 63°F, with average highs around 59°F and lows around 52°F.\\n- Mostly cloudy with some rain, with wind speeds around 15-27 mph from the south.\\n- The weather is typical for San Francisco in the winter, with cool temperatures and some precipitation.\\n\\nThe search results provide a detailed forecast for the weather in San Francisco in February 2025, including hourly temperature, wind direction, and other key weather metrics. This gives a good overview of the expected weather conditions in the city during that time period.', response_metadata={'ResponseMetadata': {'RequestId': '66d2df6b-8f89-45b3-93e0-17d035f9945f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 02 Oct 2025 07:05:19 GMT', 'content-type': 'application/json', 'content-length': '876', 'connection': 'keep-alive', 'x-amzn-requestid': '66d2df6b-8f89-45b3-93e0-17d035f9945f'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 1664}}, id='run-5ffb1ca4-c103-4345-95bb-f02e8057c8ab-0', usage_metadata={'input_tokens': 2391, 'output_tokens': 157, 'total_tokens': 2548})]\n"
     ]
    }
   ],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f625b",
   "metadata": {},
   "source": [
    "## Demonstrating Persistence\n",
    "\n",
    "To illustrate the effectiveness of our persistence implementation, we continue the conversation with a follow-up question: \"What about in LA?\". By using the same thread ID, we ensure continuity from the previous interaction. Claude maintains context, understanding that we're still inquiring about weather conditions due to the persistence provided by our checkpoint system.\n",
    "\n",
    "We can further emphasize the importance of thread ID by altering it and posing the question, \"Which one is warmer?\". With the original thread ID, Claude can accurately compare temperatures. However, changing the thread ID results in Claude losing context, as it no longer has access to the conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content=[{'type': 'tool_use', 'name': 'tavily_search_results_json', 'input': {'query': 'weather in los angeles'}, 'id': 'tooluse_ptdZY9o2RdaBYt6O0-8t7w'}], response_metadata={'ResponseMetadata': {'RequestId': '612141a9-2f52-4ee0-9f1c-a3985a5436af', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 02 Oct 2025 07:34:45 GMT', 'content-type': 'application/json', 'content-length': '333', 'connection': 'keep-alive', 'x-amzn-requestid': '612141a9-2f52-4ee0-9f1c-a3985a5436af'}, 'RetryAttempts': 0}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': 1254}}, id='run-bf79a49a-37f8-4338-a6ea-2d420d810d4e-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in los angeles'}, 'id': 'tooluse_ptdZY9o2RdaBYt6O0-8t7w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2556, 'output_tokens': 61, 'total_tokens': 2617})]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in los angeles'}, 'id': 'tooluse_ptdZY9o2RdaBYt6O0-8t7w', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content=\"[{'url': 'https://world-weather.info/forecast/usa/los_angeles/february-2025/', 'content': '* Weather * Weather in Los Angeles # Weather in Los Angeles in February 2025 Los Angeles Weather Forecast for February 2025 is based on statistical data. * +64° * +66° +63° +48° +61° +50° 7.6 mph S 29.7 inHg68 %06:47 am05:26 pm +61° +52° +59° +57° 7.4 mph S 29.7 inHg91 %06:45 am05:28 pm +59° +57° +61° +48° +66° +50° +61° +46° +57° +50° +52° +52° +55° +52° 11.2 mph S 29.6 inHg99 %06:38 am05:35 pm +59° +55° +64° +50° +70° +50° +66° +50° +63° +52° +66° +50° +68° +52° +68° +52° +72° +52° +72° +52° +68° +54° +70° +54° +77° +52° +81° +57° +64° +57° Weather in Washington, D.C.**+55°** Sacramento**+68°** Norwalk**+66°** Pasadena**+64°** Rosemead**+64°** Inglewood**+64°** Bellflower**+68°** Burbank**+64°** Compton**+66°** Beverlywood**+64°** Temperature units'}, {'url': 'https://www.timeanddate.com/weather/usa/los-angeles/historic?month=2&year=2025', 'content': 'In February 2025, Los Angeles had an average temperature of 57°F, with highs of 85°F and lows of 47°F. The average humidity was 76%.'}]\", name='tavily_search_results_json', tool_call_id='tooluse_ptdZY9o2RdaBYt6O0-8t7w')]}\n",
      "{'messages': [AIMessage(content='Based on the search results, the weather in Los Angeles in February 2025 is expected to be:\\n\\n- Temperatures ranging from around 48°F to 72°F, with average highs around 66°F and lows around 52°F.\\n- Mostly sunny with some cloud cover, with wind speeds around 7-11 mph from the south.\\n- The weather is typical for Los Angeles in the winter, with mild temperatures and low precipitation.\\n\\nThe search results provide a detailed forecast for the weather in Los Angeles in February 2025, including hourly temperature, wind direction, and other key weather metrics. This gives a good overview of the expected weather conditions in the city during that time period.\\n\\nCompared to the weather in San Francisco, Los Angeles is expected to be warmer and sunnier, with higher average temperatures and less precipitation. The coastal location of Los Angeles results in a more mild and pleasant winter climate compared to the cooler and wetter conditions in San Francisco.', response_metadata={'ResponseMetadata': {'RequestId': '97cd0d9c-9a81-4c66-a504-891b250393d4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 02 Oct 2025 07:34:52 GMT', 'content-type': 'application/json', 'content-length': '1176', 'connection': 'keep-alive', 'x-amzn-requestid': '97cd0d9c-9a81-4c66-a504-891b250393d4'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 1687}}, id='run-b2b5c977-a388-48c0-a872-011b505a2e6c-0', usage_metadata={'input_tokens': 3117, 'output_tokens': 215, 'total_tokens': 3332})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607bb30",
   "metadata": {},
   "source": [
    "## Token-Level Streaming\n",
    "\n",
    "For a more granular approach to streaming, we implement token-level updates using the `astream_events` method. This asynchronous method necessitates an async checkpointer, which we implement using `AsyncSqliteSaver`.\n",
    "\n",
    "Asynchronous programming allows our application to handle multiple operations concurrently without blocking the main execution thread. In the context of streaming tokens from an AI model, this translates to processing and displaying tokens as they're generated, resulting in a more responsive user experience. The `astream_events` method leverages this asynchronous approach to efficiently stream token-level updates from Claude.\n",
    "\n",
    "We initiate a new conversation with a fresh thread ID and iterate over the events, specifically looking for events of type \"on_chat_model_stream\". Upon encountering these events, we extract and display the content.\n",
    "\n",
    "When executed, we observe tokens streaming in real-time. We see Claude invoke the function (which doesn't generate streamable content), followed by the final response streaming token by token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'text', 'text': 'Okay', 'index': 0}]|[{'type': 'text', 'text': ', let', 'index': 0}]|[{'type': 'text', 'text': ' me', 'index': 0}]|[{'type': 'text', 'text': ' look', 'index': 0}]|[{'type': 'text', 'text': ' up the current', 'index': 0}]|[{'type': 'text', 'text': ' weather in San Francisco', 'index': 0}]|[{'type': 'text', 'text': ':', 'index': 0}]|[{'index': 0}]|[{'type': 'tool_use', 'name': 'tavily_search_results_json', 'id': 'tooluse_dgVGYV6ZQD2obaFyoLcH_g', 'index': 1}]|[{'type': 'tool_use', 'input': '', 'id': None, 'index': 1}]|[{'type': 'tool_use', 'input': '{\"query\":', 'id': None, 'index': 1}]|[{'type': 'tool_use', 'input': ' \"c', 'id': None, 'index': 1}]|[{'type': 'tool_use', 'input': 'urrent ', 'id': None, 'index': 1}]|[{'type': 'tool_use', 'input': 'we', 'id': None, 'index': 1}]|[{'type': 'tool_use', 'input': 'ather i', 'id': None, 'index': 1}]|[{'type': 'tool_use', 'input': 'n san franci', 'id': None, 'index': 1}]|[{'type': 'tool_use', 'input': 'sco\"}', 'id': None, 'index': 1}]|[{'index': 1}]|Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in san francisco'}, 'id': 'tooluse_dgVGYV6ZQD2obaFyoLcH_g', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "[{'type': 'text', 'text': '\\n\\nAccording', 'index': 0}]|[{'type': 'text', 'text': ' to', 'index': 0}]|[{'type': 'text', 'text': ' the', 'index': 0}]|[{'type': 'text', 'text': ' search', 'index': 0}]|[{'type': 'text', 'text': ' results', 'index': 0}]|[{'type': 'text', 'text': ',', 'index': 0}]|[{'type': 'text', 'text': ' the', 'index': 0}]|[{'type': 'text', 'text': ' current', 'index': 0}]|[{'type': 'text', 'text': ' weather', 'index': 0}]|[{'type': 'text', 'text': ' in', 'index': 0}]|[{'type': 'text', 'text': ' San', 'index': 0}]|[{'type': 'text', 'text': ' Francisco', 'index': 0}]|[{'type': 'text', 'text': ' is', 'index': 0}]|[{'type': 'text', 'text': ':', 'index': 0}]|[{'type': 'text', 'text': '\\n\\n-', 'index': 0}]|[{'type': 'text', 'text': ' Temperatures', 'index': 0}]|[{'type': 'text', 'text': ' ranging', 'index': 0}]|[{'type': 'text', 'text': ' from', 'index': 0}]|[{'type': 'text', 'text': ' aroun', 'index': 0}]|[{'type': 'text', 'text': 'd ', 'index': 0}]|[{'type': 'text', 'text': '55', 'index': 0}]|[{'type': 'text', 'text': '°', 'index': 0}]|[{'type': 'text', 'text': 'F', 'index': 0}]|[{'type': 'text', 'text': ' to', 'index': 0}]|[{'type': 'text', 'text': ' ', 'index': 0}]|[{'type': 'text', 'text': '68', 'index': 0}]|[{'type': 'text', 'text': '°', 'index': 0}]|[{'type': 'text', 'text': 'F', 'index': 0}]|[{'type': 'text', 'text': ',', 'index': 0}]|[{'type': 'text', 'text': ' with', 'index': 0}]|[{'type': 'text', 'text': ' an', 'index': 0}]|[{'type': 'text', 'text': ' average', 'index': 0}]|[{'type': 'text', 'text': ' high', 'index': 0}]|[{'type': 'text', 'text': ' of', 'index': 0}]|[{'type': 'text', 'text': ' aroun', 'index': 0}]|[{'type': 'text', 'text': 'd ', 'index': 0}]|[{'type': 'text', 'text': '63', 'index': 0}]|[{'type': 'text', 'text': '°', 'index': 0}]|[{'type': 'text', 'text': 'F', 'index': 0}]|[{'type': 'text', 'text': ' an', 'index': 0}]|[{'type': 'text', 'text': 'd an', 'index': 0}]|[{'type': 'text', 'text': ' average', 'index': 0}]|[{'type': 'text', 'text': ' low', 'index': 0}]|[{'type': 'text', 'text': ' of', 'index': 0}]|[{'type': 'text', 'text': ' aroun', 'index': 0}]|[{'type': 'text', 'text': 'd ', 'index': 0}]|[{'type': 'text', 'text': '52', 'index': 0}]|[{'type': 'text', 'text': '°', 'index': 0}]|[{'type': 'text', 'text': 'F', 'index': 0}]|[{'type': 'text', 'text': '.', 'index': 0}]|[{'type': 'text', 'text': '\\n-', 'index': 0}]|[{'type': 'text', 'text': ' Mostly', 'index': 0}]|[{'type': 'text', 'text': ' sunny', 'index': 0}]|[{'type': 'text', 'text': ' conditions', 'index': 0}]|[{'type': 'text', 'text': ',', 'index': 0}]|[{'type': 'text', 'text': ' with', 'index': 0}]|[{'type': 'text', 'text': ' some', 'index': 0}]|[{'type': 'text', 'text': ' occasional', 'index': 0}]|[{'type': 'text', 'text': ' pat', 'index': 0}]|[{'type': 'text', 'text': 'chy', 'index': 0}]|[{'type': 'text', 'text': ' rain', 'index': 0}]|[{'type': 'text', 'text': ' or', 'index': 0}]|[{'type': 'text', 'text': ' light', 'index': 0}]|[{'type': 'text', 'text': ' dr', 'index': 0}]|[{'type': 'text', 'text': 'izz', 'index': 0}]|[{'type': 'text', 'text': 'le', 'index': 0}]|[{'type': 'text', 'text': '.', 'index': 0}]|[{'type': 'text', 'text': '\\n-', 'index': 0}]|[{'type': 'text', 'text': ' Moderate', 'index': 0}]|[{'type': 'text', 'text': ' winds', 'index': 0}]|[{'type': 'text', 'text': ' aroun', 'index': 0}]|[{'type': 'text', 'text': 'd ', 'index': 0}]|[{'type': 'text', 'text': '10', 'index': 0}]|[{'type': 'text', 'text': '-', 'index': 0}]|[{'type': 'text', 'text': '15', 'index': 0}]|[{'type': 'text', 'text': ' ', 'index': 0}]|[{'type': 'text', 'text': 'mph', 'index': 0}]|[{'type': 'text', 'text': '.', 'index': 0}]|[{'type': 'text', 'text': '\\n\\nThe', 'index': 0}]|[{'type': 'text', 'text': ' weather', 'index': 0}]|[{'type': 'text', 'text': ' seems', 'index': 0}]|[{'type': 'text', 'text': ' to', 'index': 0}]|[{'type': 'text', 'text': ' be', 'index': 0}]|[{'type': 'text', 'text': ' fairly', 'index': 0}]|[{'type': 'text', 'text': ' mil', 'index': 0}]|[{'type': 'text', 'text': 'd an', 'index': 0}]|[{'type': 'text', 'text': 'd pleasant', 'index': 0}]|[{'type': 'text', 'text': ',', 'index': 0}]|[{'type': 'text', 'text': ' with', 'index': 0}]|[{'type': 'text', 'text': ' mostly', 'index': 0}]|[{'type': 'text', 'text': ' sunny', 'index': 0}]|[{'type': 'text', 'text': ' sk', 'index': 0}]|[{'type': 'text', 'text': 'ies', 'index': 0}]|[{'type': 'text', 'text': ' an', 'index': 0}]|[{'type': 'text', 'text': 'd comfortable', 'index': 0}]|[{'type': 'text', 'text': ' temperatures', 'index': 0}]|[{'type': 'text', 'text': ' typical', 'index': 0}]|[{'type': 'text', 'text': ' for', 'index': 0}]|[{'type': 'text', 'text': ' this', 'index': 0}]|[{'type': 'text', 'text': ' time', 'index': 0}]|[{'type': 'text', 'text': ' of', 'index': 0}]|[{'type': 'text', 'text': ' year', 'index': 0}]|[{'type': 'text', 'text': ' in', 'index': 0}]|[{'type': 'text', 'text': ' San', 'index': 0}]|[{'type': 'text', 'text': ' Francisco', 'index': 0}]|[{'type': 'text', 'text': '.', 'index': 0}]|[{'type': 'text', 'text': ' There', 'index': 0}]|[{'type': 'text', 'text': ' is', 'index': 0}]|[{'type': 'text', 'text': ' a', 'index': 0}]|[{'type': 'text', 'text': ' chance', 'index': 0}]|[{'type': 'text', 'text': ' of', 'index': 0}]|[{'type': 'text', 'text': ' some', 'index': 0}]|[{'type': 'text', 'text': ' light', 'index': 0}]|[{'type': 'text', 'text': ' rain', 'index': 0}]|[{'type': 'text', 'text': ',', 'index': 0}]|[{'type': 'text', 'text': ' but', 'index': 0}]|[{'type': 'text', 'text': ' overall', 'index': 0}]|[{'type': 'text', 'text': ' the', 'index': 0}]|[{'type': 'text', 'text': ' forecast', 'index': 0}]|[{'type': 'text', 'text': ' looks', 'index': 0}]|[{'type': 'text', 'text': ' goo', 'index': 0}]|[{'type': 'text', 'text': 'd.', 'index': 0}]|[{'index': 0}]|"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "# # If you are using a newer version of LangGraph, the package was separated:\n",
    "# # !pip install langgraph-checkpoint-sqlite\n",
    "\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "# from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(\"checkpoints.db\") as memory:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "\n",
    "    messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "    async for event in abot.graph.astream_events(\n",
    "        {\"messages\": messages}, thread, version=\"v1\"\n",
    "    ):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Empty content in the context of Amazon Bedrock means\n",
    "                # that the model is asking for a tool to be invoked.\n",
    "                # So we only print non-empty content\n",
    "                print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e0e9e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This lab has provided a comprehensive exploration of persistence and streaming implementation using Anthropic's Claude model on Amazon Bedrock. While these concepts are straightforward to implement, they offer powerful capabilities for building production-grade AI applications.\n",
    "\n",
    "The ability to manage multiple simultaneous conversations, coupled with a robust memory system for conversation resumption, is crucial for scalable AI solutions. Moreover, the capacity to stream both final tokens and intermediate messages provides unparalleled visibility into the AI's decision-making process.\n",
    "\n",
    "Persistence also plays a vital role in enabling human-in-the-loop interactions, a topic we will explore in greater depth in our subsequent lab.\n",
    "\n",
    " To gain a deeper understanding of the practical implications of these concepts, we recommend exploring real-world case studies of persistence and streaming in production AI applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df424a98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
