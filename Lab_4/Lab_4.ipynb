{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5789bc3-b1ae-42c7-94a8-2ef4f89946fc",
   "metadata": {},
   "source": [
    "# Lab 4: Persistence and Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b724f8",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We begin by establishing our agent environment. This process involves loading the necessary environment variables, importing required modules, initializing our Tavily search tool, defining the agent state, and finally, constructing our agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "# import local modules\n",
    "dir_current = os.path.abspath(\"\")\n",
    "dir_parent = os.path.dirname(dir_current)\n",
    "if dir_parent not in sys.path:\n",
    "    sys.path.append(dir_parent)\n",
    "from utils import utils\n",
    "\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Set basic configs\n",
    "logger = utils.set_logger()\n",
    "pp = utils.set_pretty_printer()\n",
    "\n",
    "# Load environment variables from .env file or Secret Manager\n",
    "_ = load_dotenv(\"../.env\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "tavily_ai_api_key = utils.get_tavily_api(\"TAVILY_API_KEY\", aws_region)\n",
    "\n",
    "# Set bedrock configs\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Create a bedrock runtime client\n",
    "bedrock_rt = boto3.client(\n",
    "    \"bedrock-runtime\", region_name=aws_region, config=bedrock_config\n",
    ")\n",
    "\n",
    "# Create a bedrock client to check available models\n",
    "bedrock = boto3.client(\"bedrock\", region_name=aws_region, config=bedrock_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06a64f-a2d5-4a66-8090-9ada0930c684",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989adc7",
   "metadata": {},
   "source": [
    "## Implementing Persistence\n",
    "\n",
    "We now turn our attention to implementing persistence. To achieve this, we introduce the concept of a checkpointer in LangGraph. The checkpointer's function is to create state snapshots after and between each node in our agent's processing graph.\n",
    "\n",
    "#RESOURCE For a more comprehensive understanding of LangGraph's capabilities and usage, refer to the official LangGraph documentation.\n",
    "\n",
    "In this implementation, we utilize a SQLite saver as our checkpointer. This lightweight solution leverages SQLite, a built-in database engine. While we use an in-memory database for this demonstration, it's important to note that this can be easily adapted to connect to an external database for production environments. LangGraph also supports other persistence solutions, including Redis and Postgres, for scenarios requiring more robust database systems.\n",
    "\n",
    "After initializing the checkpointer, we pass it to the `graph.compile` method. We've enhanced our agent to accept a `checkpointer` parameter, which we set to our memory object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01441e5e",
   "metadata": {},
   "source": [
    "## The Agent Class: A Detailed Examination\n",
    "\n",
    "The `Agent` class serves as the cornerstone of our implementation, orchestrating the interactions between the language model (Claude), tools (such as the Tavily search), and the overall conversation flow. Let's examine its key components:\n",
    "\n",
    "1. `__init__` method: This initializer sets up the agent with a model, tools, checkpointer, and an optional system message. It constructs the state graph that defines the agent's behavior.\n",
    "\n",
    "2. `call_bedrock` method: This method is responsible for invoking the Claude model via Amazon Bedrock. It processes the current state (messages) and returns the model's response.\n",
    "\n",
    "3. `exists_action` method: This method evaluates whether the latest message from the model includes any tool calls (actions to be executed).\n",
    "\n",
    "4. `take_action` method: This method executes any tool calls specified by the model and returns the results.\n",
    "\n",
    "The `Agent` class utilizes a `StateGraph` to manage the conversation flow, enabling complex interactions while maintaining a clear and manageable structure. This design choice facilitates the implementation of persistence and streaming capabilities.\n",
    "\n",
    "## Streaming Implementation\n",
    "\n",
    "With our agent now configured, we can implement streaming functionality. There are two primary aspects of streaming to consider:\n",
    "\n",
    "1. Message Streaming: This involves streaming individual messages, including the AI message that determines the next action and the observation message that represents the action's result.\n",
    "\n",
    "2. Token Streaming: This involves streaming each token of the language model's response as it's generated.\n",
    "\n",
    "We'll begin by implementing message streaming. We create a human message (e.g., \"What is the weather in SF?\") and introduce a thread config. This thread config is crucial for managing multiple conversations simultaneously within the persistent checkpointer, a necessity for production applications serving multiple users.\n",
    "\n",
    "We invoke the graph using the `stream` method instead of `invoke`, passing our messages dictionary and thread config. This returns a stream of events representing real-time updates to the state.\n",
    "\n",
    "Upon execution, we observe a stream of results: first, an AI message from Claude determining the action to take, followed by a tool message containing the Tavily search results, and finally, another AI message from Claude answering our initial query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_bedrock)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", self.exists_action, {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_bedrock(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(\n",
    "                ToolMessage(tool_call_id=t[\"id\"], name=t[\"name\"], content=str(result))\n",
    "            )\n",
    "        print(\"Back to the model!\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    client=bedrock_rt,\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f625b",
   "metadata": {},
   "source": [
    "## Demonstrating Persistence\n",
    "\n",
    "To illustrate the effectiveness of our persistence implementation, we continue the conversation with a follow-up question: \"What about in LA?\". By using the same thread ID, we ensure continuity from the previous interaction. Claude maintains context, understanding that we're still inquiring about weather conditions due to the persistence provided by our checkpoint system.\n",
    "\n",
    "We can further emphasize the importance of thread ID by altering it and posing the question, \"Which one is warmer?\". With the original thread ID, Claude can accurately compare temperatures. However, changing the thread ID results in Claude losing context, as it no longer has access to the conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607bb30",
   "metadata": {},
   "source": [
    "## Token-Level Streaming\n",
    "\n",
    "For a more granular approach to streaming, we implement token-level updates using the `astream_events` method. This asynchronous method necessitates an async checkpointer, which we implement using `AsyncSqliteSaver`.\n",
    "\n",
    "Asynchronous programming allows our application to handle multiple operations concurrently without blocking the main execution thread. In the context of streaming tokens from an AI model, this translates to processing and displaying tokens as they're generated, resulting in a more responsive user experience. The `astream_events` method leverages this asynchronous approach to efficiently stream token-level updates from Claude.\n",
    "\n",
    "We initiate a new conversation with a fresh thread ID and iterate over the events, specifically looking for events of type \"on_chat_model_stream\". Upon encountering these events, we extract and display the content.\n",
    "\n",
    "When executed, we observe tokens streaming in real-time. We see Claude invoke the function (which doesn't generate streamable content), followed by the final response streaming token by token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "# # If you are using a newer version of LangGraph, the package was separated:\n",
    "# # !pip install langgraph-checkpoint-sqlite\n",
    "\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "# from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(\"checkpoints.db\") as memory:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "\n",
    "    messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "    async for event in abot.graph.astream_events(\n",
    "        {\"messages\": messages}, thread, version=\"v1\"\n",
    "    ):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Empty content in the context of Amazon Bedrock means\n",
    "                # that the model is asking for a tool to be invoked.\n",
    "                # So we only print non-empty content\n",
    "                print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e0e9e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This lab has provided a comprehensive exploration of persistence and streaming implementation using Anthropic's Claude model on Amazon Bedrock. While these concepts are straightforward to implement, they offer powerful capabilities for building production-grade AI applications.\n",
    "\n",
    "The ability to manage multiple simultaneous conversations, coupled with a robust memory system for conversation resumption, is crucial for scalable AI solutions. Moreover, the capacity to stream both final tokens and intermediate messages provides unparalleled visibility into the AI's decision-making process.\n",
    "\n",
    "Persistence also plays a vital role in enabling human-in-the-loop interactions, a topic we will explore in greater depth in our subsequent lab.\n",
    "\n",
    " To gain a deeper understanding of the practical implications of these concepts, we recommend exploring real-world case studies of persistence and streaming in production AI applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df424a98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-dev-env",
   "language": "python",
   "name": "agents-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
